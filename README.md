# NLP_HuggingFace_Course
My notes on learning NLP HuggingFace course!
## Table of Contents
- [Chapter 1. Transformer models](#chapter-1---transformer-models)
- [Chapter 2. Using ðŸ¤— Transformers](#chapter-2---using--transformers)
- [Chapter 3. Fine-tuning a pretrained model](#chapter-3---fine-tuning-a-pretrained-model)
- [Chapter 4. Sharing models and tokenizers](#chapter-4---sharing-models-and-tokenizers)
- [Chapter 5. The ðŸ¤— Datasets library](#chapter-5---the--datasets-library)
- [Chapter 6. The ðŸ¤— Tokenizers library](#chapter-6---the--tokenizers-library)
- [Chapter 7. Main NLP tasks](#chapter-7---main-nlp-tasks)
- [Chapter 8. How to ask for help](#chapter-8---how-to-ask-for-help)
- [Chapter 9. Building and sharing demos](#chapter-9---building-and-sharing-demos)

-------------------------------------------------------------------------------------

## Chapter 1 - Transformer models
### Working with pipelines:
- The most basic object in the ðŸ¤— Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.
- There are three main steps involved when you pass some text to a pipeline:
  1. The text is preprocessed into a format the model can understand.
  2. The preprocessed inputs are passed to the model.
  3. The predictions of the model are post-processed, so you can make sense of them.
- Some of the currently available pipelines are:
  1. feature-extraction (get the vector representation of a text)
  2. fill-mask
  3. ner (named entity recognition)
  4. question-answering
  5. sentiment-analysis
  6. summarization
  7. text-generation
  8. translation
  9. zero-shot-classification
- You can see basic use of pipelines [here](working-with-pipelines.ipynb)

## Chapter 2 - Using ðŸ¤— Transformers



